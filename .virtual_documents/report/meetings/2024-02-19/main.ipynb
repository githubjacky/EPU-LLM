import os
os.chdir('../../../')


from langchain.prompts import PromptTemplate

relations = [
    'synonymous vocabularies which are nouns and related to "policy"',
    '"government occupations" which are highly related to "national policy"',
    '"government agency" which are highly related to "national policy"',
    'synonymous vocabularies which are verbs and related to "policy"',
    'the most common vocabularies that are related to policy'
]


template = """\
You are a {nationality} helpful assistant that can list {n_items} {relation}.

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    relation              = relations[4],
    nationality           = 'Taiwanese',
    n_items               = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation',
))


template = """\
You are a {nationality} helpful assistant that can list {n_items} the most common vocabularies describing economic-relevant "uncertainty".

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    nationality           = 'Taiwanese',
    n_items               = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation'
))


template = """\
You are a {nationality} {role} who reads ”economic newspaper” extensively. Your task is to list {n_items} {relation}.

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    relation              = relations[3],
    role                  = 'newspaper reader',
    nationality           = 'Taiwanese',
    n_items               = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation'
))


template = """\
You are a {nationality} {role} who reads ”economic newspaper” extensively. Your task is to list {n_items} the most common vocabularies describing economic-relevant "uncertainty".

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    role                  = 'newspaper reader',
    nationality           = 'Taiwanese',
    n_items               = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation'
))


template = """\
You are building indices of policy-related economic uncertainty based on {country} newspaper coverage frequency, \
with the aim to capture uncertainty about who will make economic policy decisions, \
what economic policy actions will be undertaken and when, \
and the economic effects of policy actions (or inaction) – including \
uncertainties related to the economic ramifications of “non-economic” policy matters, e.g., military actions.

The process of building the index is as follows:
1. Define three sets of keywords, E, P, U, containing keywords corresponding to the \
economy, policy, and uncertainty, respectively.

2. Given a collection of news articles x, an article is considered related to \
policy-related economic uncertainty if it "meets the following three criteria simultaneously":
- Contains a word belonging to the E set
- Contains a word belonging to the P set
- Contains a word belonging to the U set

3. The index is calculated as the number of news articles related to \
policy-related economic uncertainty divided by the total number of news articles in x.

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Your task is to "define and list {n_items} keywords in bullet points for each E, P, U set". 

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    country  = 'Taiwan\'s',
    n_items      = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation'
))


template = """\
You are building indices of policy-related economic uncertainty based on {country} newspaper coverage frequency, \
with the aim to capture uncertainty about who will make economic policy decisions, \
what economic policy actions will be undertaken and when, \
and the economic effects of policy actions (or inaction) – including \
uncertainties related to the economic ramifications of “non-economic” policy matters, e.g., military actions.

The process of building the index is as follows:
1. Define three sets of keywords, E, P, U, containing keywords corresponding to the \
economy, policy, and uncertainty, respectively.

2. Given a collection of news articles x, an article is considered related to \
policy-related economic uncertainty if it "meets the following three criteria simultaneously":
- Contains a word belonging to the E set
- Contains a word belonging to the P set
- Contains a word belonging to the U set

3. The index is calculated as the number of news articles related to \
policy-related economic uncertainty divided by the total number of news articles in x. \newline

"You must consider the concept of Word Segmentation and output non-compound words, but rather simple and common words."

Please proceed with the following tasks step by step.

1. "Define and list {n_items} keywords in bullet points for each E, P, U set"

2. Provide an example of a news article, including its title and content, which is related to policy-related economic uncertainty

3. Identify a word from the article in task 2 that belongs to the set defined in task 1 as E

4. Identify a word from the article in task 2 that belongs to the set defined in task 1 as P

5. Identify a word from the article in task 2 that belongs to the set defined in task 1 as U

"Warning: The words listed in tasks 3, 4, and 5 must be consistent with those you defined in task 1; \
do not list words that you did not define in the E, P, U sets in task 1."

Output Instructions:
1. List in bullet points
{language_instructions}\
"""
print(PromptTemplate.from_template(template).format(
    country  = 'Taiwan\'s',
    n_items      = 20,
    language_instructions = '2. Output only in "Traditional Chinese" and no need of English Translation'
))


import matplotlib.pyplot as plt
import orjson
from pathlib import Path
from wordcloud import WordCloud


country = 'TW'
models = ['ChatGPT-3.5', 'ChatGPT-4', 'Claude.ai-beta']
cates = ['economy', 'policy', 'uncertainty']
dir = Path(f'data/processed/auto-epu/{country}')
wc = WordCloud(
    font_path = 'report/fonts/traditional_chinese/noto_serif/regular.otf',
    background_color='white',
    colormap='BrBG'
);


fig, axs = plt.subplots(1, len(models), figsize=(20, 5))

for i in range(len(models)):
    data = orjson.loads((dir / f'{models[i]}/sc_keywords.json').read_text())
    axs[i].imshow(wc.generate_from_frequencies(data['economy']))
    axs[i].set(title=models[i])


fig, axs = plt.subplots(1, len(models), figsize=(20, 5))

for i in range(len(models)):
    data = orjson.loads((dir / f'{models[i]}/sc_keywords.json').read_text())
    axs[i].imshow(wc.generate_from_frequencies(data['policy']))
    axs[i].set(title=models[i])


fig, axs = plt.subplots(1, len(models), figsize=(20, 5))

for i in range(len(models)):
    data = orjson.loads((dir / f'{models[i]}/sc_keywords.json').read_text())
    axs[i].imshow(wc.generate_from_frequencies(data['uncertainty']))
    axs[i].set(title=models[i])


import hvplot.pandas 
import hvplot as hv
import orjson
import pandas as pd
from pathlib import Path


country = 'TW'
models = ['ChatGPT-3.5', 'ChatGPT-4', 'Claude.ai-beta']

economic_context_acc = []
policy_context_acc= []
uncertainty_context_acc = []
economic_def_acc = []
policy_def_acc = []
uncertainty_def_acc = []

def get_acc(model: str, key: str):
    lines = (
        Path(f'data/processed/auto-epu/{country}/{model}/sc_eval.jsonl')
        .read_text()
        .split('\n')[:-1]
    )
    return (
        sum([
            orjson.loads(line)[key] 
            for line in lines
        ])
        /11
    )
    
for model in models:   
    economic_context_acc.append(get_acc(model, 'is_econ_in_news'))
    policy_context_acc.append(get_acc(model, 'is_policy_in_news'))
    uncertainty_context_acc.append(get_acc(model, 'is_uncertainty_in_news'))
    economic_def_acc.append(get_acc(model, 'is_econ_in_def'))
    policy_def_acc.append(get_acc(model, 'is_policy_in_def'))
    uncertainty_def_acc.append(get_acc(model, 'is_uncertainty_in_def'))

df = pd.DataFrame({
    'model': models,
    'economic_context_acc': economic_context_acc,
    'policy_context_acc': policy_context_acc,
    'uncertainty_context_acc': uncertainty_context_acc,
    'economic_def_acc': economic_def_acc,
    'policy_def_acc': policy_def_acc,
    'uncertainty_def_acc': uncertainty_def_acc
})

df = pd.melt(df, id_vars='model', value_vars=df.columns, var_name='measure')
df.set_index(['measure', 'model'], inplace=True)


hvplot.extension('bokeh')
plot = df.hvplot.bar(xlabel = '', ylabel = 'accuracy', width=1000, rot=90)
plot


hvplot.save(plot, 'report/meetings/2024-02-19/Figures/fig10.html')


from collections import Counter
import hvplot
import hvplot.pandas
import numpy as np
import pandas as pd
from scipy.stats import entropy


def shannons_entropy_scipy(country, model, category, paradigm):
    df = pd.read_excel(
        f'data/processed/auto-epu/{country}/{model}/{category}.xlsx', 
        sheet_name = paradigm,
        header = None
    )
    words = df.values.reshape(-1,).tolist()
    count = Counter(words)
    total_tokens = sum(count.values())

    word2prob = {}
    for word in count.keys():
        word2prob[word] = count[word] / total_tokens

    entropies = []
    for col in df.columns:
        probs = []
        for word in df[col]:
            probs.append(word2prob[word])
        entropies.append(entropy(probs, base=2))

    n = len(entropies)
    return pd.DataFrame({
        'model': [model]*n,
        'category': [category]*n,
        'paradigm': [paradigm]*n,
        'entropy': entropies
    })


models = ['ChatGPT-3.5', 'ChatGPT-4', 'Claude.ai-beta']
categories = ['policy', 'uncertainty']
paradigms = ['talent', 'newspaper reader', 'newspaper editor', 'economist', 'definition', 'definition with SC']

entropy_df_policy = pd.DataFrame()
for model in models:
        for paradigm in paradigms:
            entropy_df_policy = pd.concat(
                [
                    entropy_df_policy,  
                    shannons_entropy_scipy('TW', model, 'policy', paradigm)
                ]
            )

entropy_df_uncertainty = pd.DataFrame()
for model in models:
        for paradigm in paradigms:
            entropy_df_uncertainty = pd.concat(
                [
                    entropy_df_uncertainty,  
                    shannons_entropy_scipy('TW', model, 'uncertainty', paradigm)
                ]
            )


plot = entropy_df_policy.hvplot.box(y='entropy', by='model')
plot



hvplot.extension('bokeh')
plot = entropy_df_policy.hvplot.box(y='entropy', by='paradigm')
plot
